import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, parse_qs
from colorama import init, Fore

# Colorama 초기화
init(autoreset=True)

# 세션 생성 (세션을 통해 로그인 상태를 유지)
session = requests.Session()

# 방문한 URL을 저장하는 세트
visited_urls = set()
logged_in = False  # 로그인 성공 여부를 추적하는 변수


# 폼 요소 분석하여 로그인 폼인지 판단하는 함수
def is_login_form(form):
    input_types = [
        input_tag.get("type", "text") for input_tag in form.find_all("input")
    ]

    # 필드의 타입이 하나는 반드시 'password'여야 하고, 전체 필드 수가 2~3개면 로그인 폼일 가능성이 높음
    return "password" in input_types and 2 <= len(input_types) <= 3


# 로그인 시도 함수 (로그인 성공 시 쿠키 또는 세션을 유지)
def attempt_login(url, form, depth):
    global logged_in  # 로그인 상태를 업데이트하기 위해 global 키워드를 사용
    action = form.get("action") or url
    login_url = urljoin(url, action)

    # 폼 입력 필드 파악 (아이디와 비밀번호 입력 필드 찾기)
    post_params = {
        input_tag.get("name"): input_tag.get("value", "")
        for input_tag in form.find_all("input")
        if input_tag.get("name")
    }

    # 아이디와 비밀번호가 필요한 필드에 admin/admin 입력
    post_params["id"] = "admin"
    post_params["password"] = "admin"

    # 로그인 시도
    print(f"{'    ' * depth}{Fore.YELLOW}Attempting login at: {login_url}")
    response = session.post(login_url, data=post_params)

    # 로그인 성공 여부 판단 (특정 텍스트 또는 상태 코드 등으로 확인 가능)
    if "Logout" in response.text or response.status_code == 200:
        print(f"{'    ' * depth}{Fore.GREEN}Login successful!")
        logged_in = True  # 로그인 성공 상태 업데이트

        # 로그인 후의 URL이 변경될 수 있으므로, 리디렉션 확인
        if response.history:
            print(f"{'    ' * depth}{Fore.GREEN}Redirected to: {response.url}")
        return True
    else:
        print(f"{'    ' * depth}{Fore.RED}Login failed.")
        return False


# HTML에서 링크를 추출하고 GET/POST 파라미터 및 폼 요소 파악
def get_links_and_params(url, depth):
    links = set()
    try:
        # 현재 세션을 이용해 페이지 요청
        response = session.get(url)
        soup = BeautifulSoup(response.text, "html.parser")

        # 페이지에서 GET 파라미터 분석
        parsed_url = urlparse(url)
        query_params = parse_qs(parsed_url.query)

        if query_params:
            print(f"{'    ' * depth}{Fore.GREEN}[GET] parameter: {query_params}")

        # POST 파라미터 추출 (form 태그 분석)
        forms = soup.find_all("form")
        for form in forms:
            form_method = form.get("method", "get").lower()
            if form_method == "post":
                inputs = form.find_all("input")
                post_params = {
                    input_tag.get("name"): input_tag.get("value")
                    for input_tag in inputs
                    if input_tag.get("name")
                }
                print(f"{'    ' * depth}{Fore.CYAN}[POST] parameter: {post_params}")

            # 로그인 폼일 가능성이 높은지 분석
            if is_login_form(form):
                print(
                    f"{'    ' * depth}{Fore.YELLOW}Potential login page found! (Login form detected)"
                )
                if attempt_login(url, form, depth):
                    return (
                        set()
                    )  # 로그인 성공하면 페이지 크롤링을 멈추고 재로그인할 필요 없음

        # 모든 링크 추출
        for a_tag in soup.find_all("a", href=True):
            link = urljoin(url, a_tag["href"])
            parsed_link = urlparse(link)

            # 앵커(#) 링크 무시, 중복 URL 무시, 같은 도메인의 링크만 탐색
            if (
                parsed_link.fragment == ""
                and link not in visited_urls
                and link.startswith(url)
            ):
                links.add(link)

        # 현재 페이지에서 OS Command Injection 의심 링크 확인
        check_os_command_injection(forms, url, depth)

        return links
    except requests.RequestException as e:
        print(f"{'    ' * depth}{Fore.RED}Request error accessing {url}: {e}")
        return set()
    except Exception as e:
        print(f"{'    ' * depth}{Fore.RED}Error accessing {url}: {e}")
        return set()


# OS Command Injection 의심 페이지 탐지
def check_os_command_injection(forms, url, depth):
    suspicious_keywords = ["cmd", "exec", "ping", "nslookup", "system", "sh", "bash"]
    for form in forms:
        form_text = str(form).lower()
        if any(keyword in form_text for keyword in suspicious_keywords):
            print(f"{'    ' * depth}{Fore.RED}Potential OS Command Injection at: {url}")


# 재귀적으로 URL 구조 탐색
def crawl_os_command_injection(url, depth=0):
    if url in visited_urls:  # 이미 방문한 URL은 탐색하지 않음
        return

    visited_urls.add(url)

    # URL 출력 (계층적 구조로 표시)
    print(f"{'    ' * depth}├── {Fore.WHITE}{url}")

    # URL의 모든 링크 가져오기 및 GET/POST 파라미터 파악
    links = get_links_and_params(url, depth + 1)

    # 하위 링크들에 대해 재귀적으로 크롤링
    for link in links:
        crawl_os_command_injection(link, depth + 1)


# 시작 URL 지정
start_url = "http://127.0.0.1/"  # 대상 URL을 여기에 입력
print(f"{Fore.YELLOW}Starting the crawl on {start_url}...")
crawl_os_command_injection(start_url)  # 크롤링 시작

if logged_in:
    # 로그인 성공 후 페이지 크롤링 시작
    print(f"{Fore.GREEN}Crawling after successful login...")
    crawl_os_command_injection(start_url)  # 재귀 호출하여 로그인 후 페이지 크롤링 시작
