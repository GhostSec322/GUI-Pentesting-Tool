# crawler.py
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin, parse_qs
from sql_injection import *

visited_urls = set()


def get_links_and_params(url, report):
    try:
        response = requests.get(url, timeout=10)
        soup = BeautifulSoup(response.text, "html.parser")
        links = set()
        is_vulnerable = False

        parsed_url = urlparse(url)
        query_params = parse_qs(parsed_url.query)

        if query_params:
            vulnerabilities = test_sql_injection(url, query_params, method="get")
            if vulnerabilities:
                is_vulnerable = True
                report["vulnerabilities"].extend(vulnerabilities)

        forms = soup.find_all("form")
        for form in forms:
            form_method = form.get("method", "get").lower()
            action = form.get("action")
            action_url = urljoin(url, action) if action else url

            inputs = form.find_all("input")
            post_params = {
                input_tag.get("name"): input_tag.get("value", "")
                for input_tag in inputs
                if input_tag.get("name")
            }

            if post_params:
                vulnerabilities = test_sql_injection(
                    action_url, post_params, method="post"
                )
                if vulnerabilities:
                    is_vulnerable = True
                    report["vulnerabilities"].extend(vulnerabilities)

        for a_tag in soup.find_all("a", href=True):
            link = urljoin(url, a_tag["href"])
            parsed_link = urlparse(link)

            if (
                parsed_link.fragment == ""
                and link not in visited_urls
                and link.startswith(url)
            ):
                links.add(link)

        return links, is_vulnerable
    except Exception as e:
        return set(), False


def crawl(url, depth=0, report=None):
    if url in visited_urls:
        return

    visited_urls.add(url)
    links, is_vulnerable = get_links_and_params(url, report)

    if is_vulnerable:
        for vuln in report["vulnerabilities"]:
            if vuln["url"] == url:
                print(
                    f"Vulnerability found on parameter '{vuln['parameter']}' using payload '{vuln['payload']}'"
                )

    for link in links:
        crawl(link, depth + 1, report)
