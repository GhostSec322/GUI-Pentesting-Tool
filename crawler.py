import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, parse_qs
from colorama import init, Fore

# 해당 파일은 URL에 접속하여 페이지, 파라미터 를 추출하게 됩니다.
# 출력결과
"""
├── http://127.0.0.1/
    [POST] parameter: {'keyword': None}
    ├── http://127.0.0.1/index.php?sort_column=title&sort=desc
        [GET] parameter: {'sort_column': ['title'], 'sort': ['desc']}
        [POST] parameter: {'keyword': None}
    ├── http://127.0.0.1/index.php?page=join
        [GET] parameter: {'page': ['join']}
        [POST] parameter: {'id': None, 'password1': None, 'password2': None, 'name': None, 'email': None, 'company': None}
    ├── http://127.0.0.1/index.php?sort_column=idx&sort=desc
        [GET] parameter: {'sort_column': ['idx'], 'sort': ['desc']}
        [POST] parameter: {'keyword': None}
    ├── http://127.0.0.1/index.php?page=login
        [GET] parameter: {'page': ['login']}
        [POST] parameter: {'id': None, 'password': None}
    ├── http://127.0.0.1/index.php?sort_column=regdate&sort=desc
        [GET] parameter: {'sort_column': ['regdate'], 'sort': ['desc']}
        [POST] parameter: {'keyword': None}
    ├── http://127.0.0.1/index.php?page=auth&idx=1&mode=view
        [GET] parameter: {'page': ['auth'], 'idx': ['1'], 'mode': ['view']}
        [POST] parameter: {'password': None, 'idx': '1', 'mode': 'view'}
    ├── http://127.0.0.1/index.php
        [POST] parameter: {'keyword': None}
        ├── http://127.0.0.1/index.php?sort_column=writer&sort=desc
            [GET] parameter: {'sort_column': ['writer'], 'sort': ['desc']}
            [POST] parameter: {'keyword': None}
        ├── http://127.0.0.1/index.php?page=view&idx=2
            [GET] parameter: {'page': ['view'], 'idx': ['2']}
"""
# Colorama 초기화
init(autoreset=True)

# 방문한 URL을 저장하는 세트
visited_urls = set()


# HTML에서 링크를 추출하고 GET/POST 파라미터를 파악하는 함수
def get_links_and_params(url, depth):
    try:
        response = requests.get(url)
        soup = BeautifulSoup(response.text, "html.parser")
        links = set()

        # 페이지에서 GET 파라미터 분석
        parsed_url = urlparse(url)
        query_params = parse_qs(parsed_url.query)

        if query_params:
            print(f"{'    ' * depth}{Fore.GREEN}[GET] parameter: {query_params}")

        # POST 파라미터 추출 (form 태그 분석)
        forms = soup.find_all("form")
        for form in forms:
            form_method = form.get("method", "get").lower()
            if form_method == "post":
                inputs = form.find_all("input")
                post_params = {
                    input_tag.get("name"): input_tag.get("value")
                    for input_tag in inputs
                    if input_tag.get("name")
                }
                print(f"{'    ' * depth}{Fore.CYAN}[POST] parameter: {post_params}")

        # 모든 링크 추출
        for a_tag in soup.find_all("a", href=True):
            link = urljoin(url, a_tag["href"])
            parsed_link = urlparse(link)

            # 앵커(#) 링크 무시, 중복 URL 무시, 같은 도메인의 링크만 탐색
            if (
                parsed_link.fragment == ""
                and link not in visited_urls
                and link.startswith(url)
            ):
                links.add(link)
        return links
    except Exception as e:
        print(f"{'    ' * depth}{Fore.RED}Error accessing {url}: {e}")
        return set()


# 재귀적으로 URL 구조 탐색
def crawl(url, depth=0):
    if url in visited_urls:  # 이미 방문한 URL은 탐색하지 않음
        return

    visited_urls.add(url)

    # URL 출력 (계층적 구조로 표시)
    print(f"{'    ' * depth}├── {Fore.WHITE}{url}")

    # URL의 모든 링크 가져오기 및 GET/POST 파라미터 파악
    links = get_links_and_params(url, depth + 1)

    # 하위 링크들에 대해 재귀적으로 크롤링
    for link in links:
        crawl(link, depth + 1)


# 시작 URL 지정
start_url = "http://127.0.0.1/"  # 대상 URL을 여기에 입력
crawl(start_url)
