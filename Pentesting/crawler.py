import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, parse_qs
from colorama import init, Fore

# Colorama 초기화
init(autoreset=True)


def get_links_and_params(url, depth):
    try:
        response = requests.get(url)
        soup = BeautifulSoup(response.text, "html.parser")
        links = set()

        # 페이지에서 GET 파라미터 분석
        parsed_url = urlparse(url)
        query_params = parse_qs(parsed_url.query)

        if query_params:
            print(f"{'│   ' * depth}{Fore.GREEN}[GET] parameter: {query_params}")

        # POST 파라미터 추출 (form 태그 분석)
        forms = soup.find_all("form")
        for form in forms:
            form_method = form.get("method", "get").lower()
            if form_method == "post":
                inputs = form.find_all("input")
                post_params = {
                    input_tag.get("name"): input_tag.get("value")
                    for input_tag in inputs
                    if input_tag.get("name")
                }
                print(f"{'│   ' * depth}{Fore.CYAN}[POST] parameter: {post_params}")

        # 모든 링크 추출
        for a_tag in soup.find_all("a", href=True):
            link = urljoin(url, a_tag["href"])
            parsed_link = urlparse(link)

            # 앵커(#) 링크 무시, 중복 URL 무시, 같은 도메인의 링크만 탐색
            if (
                parsed_link.fragment == ""
                and link not in visited_urls
                and link.startswith(url)
            ):
                links.add(link)
        return links
    except Exception as e:
        print(f"{'│   ' * depth}{Fore.RED}Error accessing {url}: {e}")
        return set()


def crawl(url, depth=0):
    if url in visited_urls:  # 이미 방문한 URL은 탐색하지 않음
        return

    visited_urls.add(url)

    # URL 출력 (계층적 구조로 표시)
    if depth == 0:
        print(f"└── {Fore.WHITE}{url}")
    else:
        print(f"{'│   ' * (depth-1)}├── {Fore.WHITE}{url}")

    # URL의 모든 링크 가져오기 및 GET/POST 파라미터 파악
    links = get_links_and_params(url, depth + 1)

    # 하위 링크들에 대해 재귀적으로 크롤링
    for i, link in enumerate(links):
        if i == len(links) - 1:
            print(f"{'│   ' * depth}└──")
        crawl(link, depth + 1)


# 방문한 URL을 저장하는 세트
visited_urls = set()
# 시작 URL 지정
start_url = "http://127.0.0.1/"  # 대상 URL을 여기에 입력
crawl(start_url)
