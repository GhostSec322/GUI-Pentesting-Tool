import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, parse_qs
from colorama import init, Fore
from Pentesting.vulnerability_tester.payload import sql_injection_payload

# Colorama 초기화
init(autoreset=True)

# 방문한 URL을 저장하는 세트
visited_urls = set()

# SQL 인젝션 페이로드 목록
payloads = sql_injection_payload()

# HTTP 요청 타임아웃 설정
timeout = 10  # 10초 후 타임아웃


# SQL 인젝션 취약점 테스트 함수
def test_sql_injection(url, params, method="get"):
    vulnerable_params = []  # 취약한 파라미터 목록
    try:
        original_response = requests.get(
            url, params=params, timeout=timeout
        ).text  # 원래 응답 저장
        for key in params.keys():
            original_value = params[key]
            for payload in payloads:
                injected_params = params.copy()
                injected_params[key] = payload
                try:
                    if method == "get":
                        response = requests.get(
                            url, params=injected_params, timeout=timeout
                        )
                    elif method == "post":
                        headers = {"Content-Type": "application/x-www-form-urlencoded"}
                        response = requests.post(
                            url, data=injected_params, headers=headers, timeout=timeout
                        )

                    # 응답 크기 또는 에러 메시지 비교
                    if (
                        len(response.text) != len(original_response)
                        or "SQL" in response.text
                    ):
                        print(
                            Fore.RED + f"[!] Possible SQL Injection: {key} = {payload}"
                        )
                        if key not in vulnerable_params:
                            vulnerable_params.append(key)  # 취약한 파라미터 기록
                    else:
                        print(Fore.GREEN + f"[+] Passed: {key} = {payload}")
                except requests.exceptions.RequestException as e:
                    print(Fore.RED + f"Error testing {key} with payload {payload}: {e}")

            # 원래 값으로 복원
            params[key] = original_value
    except requests.exceptions.RequestException as e:
        print(Fore.RED + f"Error accessing {url}: {e}")
    return vulnerable_params  # 취약한 파라미터 목록 반환


# HTML에서 링크를 추출하고 GET/POST 파라미터를 파악하는 함수
def get_links_and_params(url):
    try:
        response = requests.get(url, timeout=timeout)
        soup = BeautifulSoup(response.text, "html.parser")
        links = set()
        get_vulnerable_params = {}  # GET 방식의 취약한 파라미터 딕셔너리
        post_vulnerable_params = {}  # POST 방식의 취약한 파라미터 딕셔너리
        all_params = {}  # 모든 파라미터 저장

        # 페이지에서 GET 파라미터 분석
        parsed_url = urlparse(url)
        query_params = parse_qs(parsed_url.query)
        if query_params:
            all_params.update(query_params)
            vulnerable = test_sql_injection(url, query_params, method="get")
            for param in query_params:
                if param in vulnerable:
                    get_vulnerable_params[param] = "Vulnerable"
                else:
                    get_vulnerable_params[param] = "Safe"

        # POST 파라미터 추출 (form 태그 분석)
        forms = soup.find_all("form")
        for form in forms:
            form_method = form.get("method", "get").lower()
            action = form.get("action")
            if action:
                action_url = urljoin(url, action)
            else:
                action_url = url
            inputs = form.find_all("input")
            post_params = {
                input_tag.get("name"): input_tag.get("value")
                for input_tag in inputs
                if input_tag.get("name")
            }
            if post_params:
                all_params.update(post_params)
                if form_method == "post":
                    vulnerable = test_sql_injection(
                        action_url, post_params, method="post"
                    )
                    for param in post_params:
                        if param in vulnerable:
                            post_vulnerable_params[param] = "Vulnerable"
                        else:
                            post_vulnerable_params[param] = "Safe"

        # 모든 링크 추출
        for a_tag in soup.find_all("a", href=True):
            link = urljoin(url, a_tag["href"])
            parsed_link = urlparse(link)
            # 앵커(#) 링크 무시, 중복 URL 무시, 같은 도메인의 링크만 탐색
            if (
                parsed_link.fragment == ""
                and link not in visited_urls
                and link.startswith(url)
            ):
                links.add(link)

        # GET/POST 파라미터 상태 출력
        print(Fore.CYAN + f"\n[+] Parameters found on page: {all_params}")
        if get_vulnerable_params:
            print(Fore.YELLOW + "GET Parameters status:")
            for param, status in get_vulnerable_params.items():
                print(f"    {param}: {status}")
        if post_vulnerable_params:
            print(Fore.YELLOW + "POST Parameters status:")
            for param, status in post_vulnerable_params.items():
                print(f"    {param}: {status}")

        return links
    except Exception as e:
        print(Fore.RED + f"Error accessing {url}: {e}")
        return set()


# 재귀적으로 URL 구조 탐색
def crawl(url, depth=0):
    if url in visited_urls:  # 이미 방문한 URL은 탐색하지 않음
        return
    visited_urls.add(url)
    # URL 출력 (계층적 구조로 표시)
    print(f"{'    ' * depth}├── {Fore.WHITE}{url}")
    # URL의 모든 링크 가져오기 및 GET/POST 파라미터 파악
    links = get_links_and_params(url)

    # 하위 링크들에 대해 재귀적으로 크롤링
    for link in links:
        crawl(link, depth + 1)


# 시작 URL 지정
start_url = "http://127.0.0.1:8000/admin"  # 대상 URL을 여기에 입력
crawl(start_url)
